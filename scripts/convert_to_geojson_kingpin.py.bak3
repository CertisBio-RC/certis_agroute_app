#!/usr/bin/env python3
"""
CERTIS AGROUTE — Build public/data/kingpin.geojson (Hybrid)
  • Join/Enrich from retailers.geojson first (Category + facility coords if matched)
  • Geocode ONLY unmatched rows (TBD) using Mapbox
  • Token resolution supports (robust):
      - --token-file (txt or json; BOM-safe)
      - data/token.txt (raw token OR JSON)
      - data/token.json (BOM-safe)
      - env MAPBOX_TOKEN or NEXT_PUBLIC_MAPBOX_TOKEN
  • Category rules (updated):
      - Facility categories are canonicalized; any "nan"/"NaN"/"None"/"null" are treated as missing
      - Missing/invalid facility categories default to "Agronomy"
      - Unmatched kingpins (TBD) always default Category to "Agronomy"

Outputs:
  - public/data/kingpin.geojson
  - scripts/out/kingpin_enriched.csv
  - scripts/out/kingpin_unmatched.csv
  - scripts/out/kingpin_build_stats.json
"""

from __future__ import annotations

import argparse
import csv
import json
import os
import re
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

# deps: pip install pandas openpyxl requests
import pandas as pd
import requests

MAPBOX_GEOCODE_URL = "https://api.mapbox.com/geocoding/v5/mapbox.places/{query}.json"

KINGPIN_REQUIRED_MIN = ["Retailer", "ContactName", "Address", "City", "State", "Zip"]

_WS_RE = re.compile(r"\s+")
_PUNCT_RE = re.compile(r"[^\w\s]")
DASH_STATE_SUFFIX_RE = re.compile(r"\s*-\s*([A-Za-z]{2}(?:\s+[A-Za-z]{2})*)\s*$")

TOKEN_MAP = {
    "hwy": "highway",
    "highwy": "highway",
    "rd": "road",
    "st": "street",
    "ave": "avenue",
    "blvd": "boulevard",
    "ln": "lane",
    "dr": "drive",
    "ct": "court",
    "trl": "trail",
    "pkwy": "parkway",
}

ORDINAL_WORDS = {
    "first": "1st",
    "second": "2nd",
    "third": "3rd",
    "fourth": "4th",
    "fifth": "5th",
    "sixth": "6th",
    "seventh": "7th",
    "eighth": "8th",
    "ninth": "9th",
    "tenth": "10th",
}

# -----------------------------------------------------------------------------
# Normalization helpers
# -----------------------------------------------------------------------------
def _clean_ws(s: str) -> str:
    return _WS_RE.sub(" ", str(s or "").strip())

def _strip_quotes(s: str) -> str:
    s = _clean_ws(s)
    if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
        return s[1:-1].strip()
    return s

def normalize_state(s: str) -> str:
    return _clean_ws(s).upper()

def normalize_zip(s: str) -> str:
    return _clean_ws(s)

def normalize_city(s: str) -> str:
    s = _clean_ws(s).lower()
    s = _PUNCT_RE.sub(" ", s)
    return _clean_ws(s)

def normalize_retailer(raw: str) -> str:
    """
    IMPORTANT: This is for MATCHING KEYS, not display.
    We intentionally normalize common cooperative variants to reduce false TBD.
    """
    s = _clean_ws(raw)
    s = DASH_STATE_SUFFIX_RE.sub("", s)  # remove trailing " - IA" etc
    s = s.lower()
    s = s.replace("&", " and ")
    s = _PUNCT_RE.sub(" ", s)
    s = _clean_ws(s)

    # normalize common coop words (safe for matching keys)
    tokens = s.split(" ")
    out = []
    for t in tokens:
        if t in ("co", "co-op", "coop", "cooperative", "cooperatives"):
            out.append("cooperative")
        elif t in ("inc", "incorporated"):
            out.append("inc")
        elif t in ("company", "co."):
            out.append("co")
        else:
            out.append(t)
    return _clean_ws(" ".join(out))

def normalize_address(raw: str) -> str:
    s = _clean_ws(raw).lower()
    s = _PUNCT_RE.sub(" ", s)
    s = _clean_ws(s)

    tokens = s.split(" ")
    tokens = [ORDINAL_WORDS.get(t, t) for t in tokens]
    tokens = [TOKEN_MAP.get(t, t) for t in tokens]
    return _clean_ws(" ".join(tokens))

def canonical_suppliers(raw: str) -> str:
    s = _clean_ws(raw)
    if not s:
        return ""
    parts = re.split(r"[;,]", s)
    parts = [_clean_ws(p) for p in parts if _clean_ws(p)]
    return ", ".join(parts)

def canonicalize_category(raw: str) -> str:
    """
    Canonical categories (Bailey Rule set):
      Agronomy, Grain/Feed, C-Store/Service/Energy, Distribution, Corporate HQ, Kingpin
    We treat messy combos (e.g. "Agronomy, Feed/Grain") by choosing best match.

    IMPORTANT: 'nan', 'NaN', 'None', 'null' are treated as missing (return "").
    """
    s = _clean_ws(raw)
    if not s:
        return ""
    low = s.lower()

    # treat these as missing
    if low in {"nan", "none", "null"}:
        return ""

    # If multiple categories are stuffed together, we still choose a single canonical.
    # Priority: Corporate HQ > Distribution > C-Store/Service/Energy > Grain/Feed > Agronomy > Kingpin
    # (Kingpin usually won't exist in retailers.geojson, but we keep it here for completeness.)
    if "corporate" in low or "hq" in low:
        return "Corporate HQ"
    if "distribution" in low:
        return "Distribution"
    if "c-store" in low or "c store" in low or "service" in low or "energy" in low:
        return "C-Store/Service/Energy"
    if "grain" in low or "feed" in low:
        return "Grain/Feed"
    if "agronomy" in low:
        return "Agronomy"
    if "kingpin" in low:
        return "Kingpin"

    # fallback: preserve original cleaned label (but avoid commas)
    if "," in s:
        s = s.split(",")[0].strip()
    return s

def is_blank_row(row: Dict[str, Any]) -> bool:
    core = ["Retailer", "ContactName", "Address", "City", "State", "Zip", "Email", "OfficePhone", "CellPhone"]
    return all(not _clean_ws(row.get(k, "")) for k in core)

def has_min_required(row: Dict[str, Any]) -> bool:
    return all(bool(_clean_ws(row.get(k, ""))) for k in KINGPIN_REQUIRED_MIN)

# -----------------------------------------------------------------------------
# Index keys
# -----------------------------------------------------------------------------
def address_key(retailer: str, address: str, city: str, state: str, zipc: str) -> str:
    return f"{retailer}|{address}|{city}|{state}|{zipc}"

def city_key(retailer: str, city: str, state: str, zipc: str) -> str:
    return f"{retailer}|{city}|{state}|{zipc}"

def retailer_state_key(retailer: str, state: str) -> str:
    return f"{retailer}|{state}"

# -----------------------------------------------------------------------------
# Facility model + loaders
# -----------------------------------------------------------------------------
@dataclass
class Facility:
    retailer_raw: str
    retailer_norm: str
    name: str
    address_raw: str
    address_norm: str
    city_raw: str
    city_norm: str
    state: str
    zipc: str
    category: str
    suppliers: str
    longname: str
    lon: float
    lat: float

def load_retailers_geojson(path: str) -> List[Facility]:
    with open(path, "r", encoding="utf-8") as f:
        doc = json.load(f)

    out: List[Facility] = []
    for feat in doc.get("features", []):
        if (feat or {}).get("type") != "Feature":
            continue
        geom = (feat or {}).get("geometry") or {}
        if geom.get("type") != "Point":
            continue
        coords = geom.get("coordinates") or []
        if not isinstance(coords, list) or len(coords) != 2:
            continue

        try:
            lon, lat = float(coords[0]), float(coords[1])
        except Exception:
            continue

        props = (feat or {}).get("properties") or {}

        r_raw = _clean_ws(props.get("Retailer", ""))
        a_raw = _clean_ws(props.get("Address", ""))
        c_raw = _clean_ws(props.get("City", ""))
        st = normalize_state(props.get("State", ""))
        z = normalize_zip(props.get("Zip", ""))

        # canonicalize, but allow blank / 'nan' to fall back to Agronomy later
        cat_raw = props.get("Category", "")
        cat = canonicalize_category(cat_raw)
        supp = canonical_suppliers(props.get("Suppliers", ""))
        name = _clean_ws(props.get("Name", ""))
        longname = _clean_ws(props.get("LongName", ""))

        # Require the fields we need for matching
        if not (r_raw and a_raw and c_raw and st and z):
            continue

        # If category is missing/invalid (nan/null/etc.), default to Agronomy.
        if not cat:
            cat = "Agronomy"

        out.append(
            Facility(
                retailer_raw=r_raw,
                retailer_norm=normalize_retailer(r_raw),
                name=name,
                address_raw=a_raw,
                address_norm=normalize_address(a_raw),
                city_raw=c_raw,
                city_norm=normalize_city(c_raw),
                state=st,
                zipc=z,
                category=cat,
                suppliers=supp,
                longname=longname,
                lon=lon,
                lat=lat,
            )
        )
    return out

def build_facility_indexes(
    facilities: List[Facility],
) -> Tuple[Dict[str, List[Facility]], Dict[str, List[Facility]], Dict[str, List[Facility]]]:
    by_addr: Dict[str, List[Facility]] = {}
    by_city: Dict[str, List[Facility]] = {}
    by_retailer_state: Dict[str, List[Facility]] = {}

    for f in facilities:
        ak = address_key(f.retailer_norm, f.address_norm, f.city_norm, f.state, f.zipc)
        by_addr.setdefault(ak, []).append(f)

        ck = city_key(f.retailer_norm, f.city_norm, f.state, f.zipc)
        by_city.setdefault(ck, []).append(f)

        rk = retailer_state_key(f.retailer_norm, f.state)
        by_retailer_state.setdefault(rk, []).append(f)

    return by_addr, by_city, by_retailer_state

def choose_best_facility(cands: List[Facility]) -> Facility:
    """
    Choose a stable default when multiple facilities match.
    Preference:
      1) Corporate HQ (best for "retailer+state" fallback)
      2) Longer/more specific address
      3) Longer name/longname
    """
    def score(f: Facility) -> Tuple[int, int, int]:
        is_hq = 1 if f.category.lower() == "corporate hq".lower() else 0
        return (is_hq, len(f.address_raw or ""), max(len(f.name or ""), len(f.longname or "")))

    return sorted(cands, key=score, reverse=True)[0]

# -----------------------------------------------------------------------------
# Kingpin XLSX reader
# -----------------------------------------------------------------------------
def read_kingpins_xlsx(path: str) -> List[Dict[str, Any]]:
    xls = pd.ExcelFile(path)
    rows: List[Dict[str, Any]] = []

    for sh in xls.sheet_names:
        if sh.strip().startswith("_"):
            continue
        df = pd.read_excel(path, sheet_name=sh, dtype=str).fillna("")
        df.columns = [str(c).strip() for c in df.columns]

        # ------------------------------------------------------------
        # Header normalization (Kingpin XLSX variants -> canonical keys)
        # Accepts headers like 'CONTACT NAME', 'ZIP CODE', 'OFFICE PHONE', etc.
        # ------------------------------------------------------------
        col_map = {}
        for c in df.columns:
            cu = str(c).strip().upper()
            if cu == 'RETAILER':
                col_map[c] = 'Retailer'
            elif cu in ('CONTACT NAME', 'CONTACTNAME'):
                col_map[c] = 'ContactName'
            elif cu == 'ADDRESS':
                col_map[c] = 'Address'
            elif cu == 'CITY':
                col_map[c] = 'City'
            elif cu in ('STATE.1', 'STATE'):
                col_map[c] = 'State'
            elif cu in ('ZIP', 'ZIP CODE', 'ZIPCODE'):
                col_map[c] = 'Zip'
            elif cu in ('OFFICE PHONE', 'OFFICEPHONE'):
                col_map[c] = 'OfficePhone'
            elif cu in ('CELL PHONE', 'CELLPHONE'):
                col_map[c] = 'CellPhone'
            elif cu == 'EMAIL':
                col_map[c] = 'Email'
            elif cu in ('TITLE', 'CONTACT TITLE', 'CONTACTTITLE'):
                col_map[c] = 'ContactTitle'
            elif cu in ('SUPPLIER', 'SUPPLIERS'):
                col_map[c] = 'Suppliers'
            elif cu in ('FULL BLOCK ADDRESS', 'FULLADDRESS', 'FULL ADDRESS'):
                col_map[c] = 'FullAddress'

        if col_map:
            df = df.rename(columns=col_map)
            # If both STATE and STATE.1 existed, keep the last (STATE.1 usually).
            if 'State' in df.columns:
                cols = list(df.columns)
                idxs = [k for k, x in enumerate(cols) if x == 'State']
                if len(idxs) > 1:
                    df = df.drop(df.columns[idxs[:-1]], axis=1)

        for _, r in df.iterrows():
            def _canon_key(_k: str) -> str:
                ku = str(_k).strip().upper()
                if ku == 'RETAILER': return 'Retailer'
                if ku in ('CONTACT NAME','CONTACTNAME'): return 'ContactName'
                if ku == 'ADDRESS': return 'Address'
                if ku == 'CITY': return 'City'
                if ku in ('STATE.1','STATE'): return 'State'
                if ku in ('ZIP','ZIP CODE','ZIPCODE'): return 'Zip'
                if ku in ('OFFICE PHONE','OFFICEPHONE'): return 'OfficePhone'
                if ku in ('CELL PHONE','CELLPHONE'): return 'CellPhone'
                if ku == 'EMAIL': return 'Email'
                if ku in ('TITLE','CONTACT TITLE','CONTACTTITLE'): return 'ContactTitle'
                if ku in ('SUPPLIER','SUPPLIERS'): return 'Suppliers'
                if ku in ('FULL BLOCK ADDRESS','FULLADDRESS','FULL ADDRESS'): return 'FullAddress'
                return str(_k).strip()

            d = { _canon_key(k): ('' if pd.isna(v) else str(v)) for k, v in r.to_dict().items() }
            d["_Sheet"] = sh
            rows.append(d)

    return rows

# -----------------------------------------------------------------------------
# File IO helpers
# -----------------------------------------------------------------------------
def ensure_out_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)

def write_csv(path: str, rows: List[Dict[str, Any]], fieldnames: List[str]) -> None:
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        w.writeheader()
        for r in rows:
            w.writerow(r)

def build_geojson_feature(lon: float, lat: float, props: Dict[str, Any]) -> Dict[str, Any]:
    return {"type": "Feature", "geometry": {"type": "Point", "coordinates": [lon, lat]}, "properties": props}

# -----------------------------------------------------------------------------
# Token resolution
# -----------------------------------------------------------------------------
def _extract_token_from_obj(obj: Any) -> str:
    if isinstance(obj, str):
        return _strip_quotes(obj)
    if isinstance(obj, dict):
        for k in ["MAPBOX_TOKEN", "token", "access_token", "NEXT_PUBLIC_MAPBOX_TOKEN"]:
            if k in obj and obj[k]:
                return _strip_quotes(str(obj[k]))
    return ""

def load_token_from_json(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8-sig") as f:  # BOM-safe
            obj = json.load(f)
        return _extract_token_from_obj(obj)
    except Exception:
        return ""

def load_token_from_txt(path: str) -> str:
    """
    Supports:
      - raw token (pk....)
      - token=pk....
      - JSON string: {"MAPBOX_TOKEN":"pk...."}  (even if saved in .txt)
    Also BOM-safe via utf-8-sig.
    """
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            raw = f.read().strip()
        raw = _strip_quotes(raw)

        if raw.startswith("{") and raw.endswith("}"):
            try:
                obj = json.loads(raw)
                t = _extract_token_from_obj(obj)
                if t:
                    return t
            except Exception:
                pass

        raw = re.sub(
            r"^\s*(token|access_token|mapbox_token|next_public_mapbox_token)\s*=\s*",
            "",
            raw,
            flags=re.IGNORECASE,
        ).strip()
        return raw
    except Exception:
        return ""

def resolve_mapbox_token(token_file: Optional[str]) -> Tuple[str, str]:
    if token_file:
        if token_file.lower().endswith(".json"):
            t = load_token_from_json(token_file)
        else:
            t = load_token_from_txt(token_file)
        if t:
            return t, f"--token-file:{token_file}"

    if os.path.exists(os.path.join("data", "token.txt")):
        t = load_token_from_txt(os.path.join("data", "token.txt"))
        if t:
            return t, "data/token.txt"
    if os.path.exists(os.path.join("data", "token.json")):
        t = load_token_from_json(os.path.join("data", "token.json"))
        if t:
            return t, "data/token.json"

    t = os.getenv("MAPBOX_TOKEN") or os.getenv("NEXT_PUBLIC_MAPBOX_TOKEN") or ""
    if t:
        return t, "env"
    return "", "none"

# -----------------------------------------------------------------------------
# Geocode cache
# -----------------------------------------------------------------------------
def load_geocode_cache(path: str) -> Dict[str, List[float]]:
    try:
        if not os.path.exists(path):
            return {}
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        if isinstance(obj, dict):
            return obj
        return {}
    except Exception:
        return {}

def save_geocode_cache(path: str, cache: Dict[str, List[float]]) -> None:
    try:
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def cache_key_full(address: str, city: str, state: str, zipc: str) -> str:
    return _clean_ws(f"{address}, {city}, {state} {zipc}").lower()

# -----------------------------------------------------------------------------
# Mapbox geocoding
# -----------------------------------------------------------------------------
def geocode_address_mapbox(
    token: str,
    address: str,
    city: str,
    state: str,
    zipc: str,
    *,
    country: str = "US",
    limit: int = 1,
    sleep_s: float = 0.12,
) -> Tuple[Optional[Tuple[float, float]], int, str]:
    query = f"{address}, {city}, {state} {zipc}"
    url = MAPBOX_GEOCODE_URL.format(query=requests.utils.quote(query))
    params = {"access_token": token, "country": country, "limit": str(limit)}

    try:
        r = requests.get(url, params=params, timeout=25)
        status = r.status_code
        if status != 200:
            return None, status, (r.text or "").strip()[:180]
        data = r.json()
        feats = data.get("features", [])
        if not feats:
            return None, status, "NO_FEATURES"
        center = feats[0].get("center")
        if not center or len(center) != 2:
            return None, status, "BAD_CENTER"
        lon, lat = float(center[0]), float(center[1])
        if sleep_s > 0:
            time.sleep(sleep_s)
        return (lon, lat), status, "OK"
    except Exception as e:
        return None, 0, f"EXCEPTION: {type(e).__name__}"

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main() -> int:
    ap = argparse.ArgumentParser(description="Build kingpin.geojson (inherit coords if matched; geocode only TBD).")
    ap.add_argument("--kingpin-xlsx", default="data/kingpin1_COMBINED.xlsx", help="Path to kingpin xlsx.")
    ap.add_argument(
        "--retailers-geojson",
        default=os.path.join("public", "data", "retailers.geojson"),
        help="Path to retailers.geojson (inheritance authority).",
    )
    ap.add_argument(
        "--out-geojson",
        default=os.path.join("public", "data", "kingpin.geojson"),
        help="Output path for kingpin.geojson.",
    )
    ap.add_argument("--out-dir", default=os.path.join("scripts", "out"), help="Directory for audit outputs.")
    ap.add_argument(
        "--geocode-no-match",
        action="store_true",
        help="If set, TBD rows will be geocoded via Mapbox. Otherwise TBD rows are excluded.",
    )
    ap.add_argument("--token-file", default="", help="Optional token file path (txt or json). BOM-safe; txt may be JSON.")
    ap.add_argument("--geocode-sleep", type=float, default=0.12, help="Sleep seconds between Mapbox calls.")
    ap.add_argument("--debug-geocode-failures", type=int, default=5, help="Print first N geocode failures.")
    ap.add_argument("--cache-file", default=os.path.join("data", "geocode-cache.json"), help="Geocode cache file.")
    args = ap.parse_args()

    if not os.path.exists(args.retailers_geojson):
        print(f"❌ Missing retailers geojson: {args.retailers_geojson}")
        return 1
    if not os.path.exists(args.kingpin_xlsx):
        print(f"❌ Missing kingpin xlsx: {args.kingpin_xlsx}")
        return 1

    facilities = load_retailers_geojson(args.retailers_geojson)
    by_addr, by_city, by_retailer_state = build_facility_indexes(facilities)
    raw_rows = read_kingpins_xlsx(args.kingpin_xlsx)

    token = ""
    token_source = "n/a"
    if args.geocode_no_match:
        token, token_source = resolve_mapbox_token(args.token_file)
        if not token:
            print("❌ geocode-no-match enabled but no token found.")
            print("   Put token in data/token.txt or data/token.json, set MAPBOX_TOKEN/NEXT_PUBLIC_MAPBOX_TOKEN, or pass --token-file.")
            return 1

    cache = load_geocode_cache(args.cache_file) if args.geocode_no_match else {}
    cache_hits = 0
    cache_writes = 0

    enriched: List[Dict[str, Any]] = []
    unmatched: List[Dict[str, Any]] = []

    n_total = 0
    n_dropped_blank = 0
    n_dropped_missing_min = 0
    n_t1 = 0
    n_t2 = 0
    n_t3 = 0
    n_tbd = 0
    n_geocoded = 0
    n_geocode_failed = 0

    fail_status_counts: Dict[str, int] = {}
    printed_failures = 0

    for row in raw_rows:
        if is_blank_row(row):
            n_dropped_blank += 1
            continue
        if not has_min_required(row):
            n_dropped_missing_min += 1
            continue

        n_total += 1

        retailer_raw = _clean_ws(row.get("Retailer", ""))
        suppliers_raw = canonical_suppliers(row.get("Suppliers", ""))
        contact = _clean_ws(row.get("ContactName", ""))
        title = _clean_ws(row.get("Title", ""))
        email = _clean_ws(row.get("Email", ""))
        office = _clean_ws(row.get("OfficePhone", ""))
        cell = _clean_ws(row.get("CellPhone", ""))
        addr_raw = _clean_ws(row.get("Address", ""))
        city_raw = _clean_ws(row.get("City", ""))
        state = normalize_state(row.get("State", ""))
        zipc = normalize_zip(row.get("Zip", ""))
        fulladdr = _clean_ws(row.get("FullAddress", ""))

        r_norm = normalize_retailer(retailer_raw)
        a_norm = normalize_address(addr_raw)
        c_norm = normalize_city(city_raw)

        ak = address_key(r_norm, a_norm, c_norm, state, zipc)
        ck = city_key(r_norm, c_norm, state, zipc)
        rk = retailer_state_key(r_norm, state)

        match_tier = "TBD"
        geosource = "MAPBOX"
        matched_fac: Optional[Facility] = None

        # T1: exact addr match (retailer+addr+city+state+zip)
        cands1 = by_addr.get(ak, [])
        if cands1:
            matched_fac = choose_best_facility(cands1)
            match_tier = "T1"
            geosource = "FACILITY"
            n_t1 += 1

        # T2: unique city match (retailer+city+state+zip) where only 1 facility in that city+zip
        if matched_fac is None:
            cands2 = by_city.get(ck, [])
            if len(cands2) == 1:
                matched_fac = cands2[0]
                match_tier = "T2"
                geosource = "FACILITY"
                n_t2 += 1

        # T3: retailer+state fallback (pick Corporate HQ if present)
        if matched_fac is None:
            cands3 = by_retailer_state.get(rk, [])
            if cands3:
                matched_fac = choose_best_facility(cands3)
                match_tier = "T3"
                geosource = "FACILITY"
                n_t3 += 1

        props: Dict[str, Any] = {
            "Retailer": retailer_raw,
            "Suppliers": suppliers_raw,
            "ContactName": contact,
            "ContactTitle": title,  # keep for popup fields
            "Title": title,         # backward compat if something expects Title
            "Email": email,
            "OfficePhone": office,
            "CellPhone": cell,
            "Address": addr_raw,
            "City": city_raw,
            "State": state,
            "Zip": zipc,
        }
        if fulladdr:
            props["FullAddress"] = fulladdr

        lonlat: Optional[Tuple[float, float]] = None

        if matched_fac is not None:
            props["Category"] = matched_fac.category  # already cleaned / defaulted away from "nan"
            props["FacilityName"] = matched_fac.name
            props["LongName"] = matched_fac.longname
            props["MatchTier"] = match_tier
            props["GeoSource"] = geosource

            fac_supp = canonical_suppliers(matched_fac.suppliers)
            if fac_supp and fac_supp.upper() != "TBD":
                props["Suppliers"] = fac_supp

            lonlat = (matched_fac.lon, matched_fac.lat)

        else:
            # Unmatched => TBD values (then geocode if enabled)
            n_tbd += 1
            # Category for unmatched kingpins: always Agronomy (per your preference)
            props["Category"] = "Agronomy"
            props["FacilityName"] = ""
            props["LongName"] = ""
            props["MatchTier"] = "TBD"
            props["GeoSource"] = "MAPBOX"

            if args.geocode_no_match:
                ck_full = cache_key_full(addr_raw, city_raw, state, zipc)
                if ck_full in cache and isinstance(cache[ck_full], list) and len(cache[ck_full]) == 2:
                    try:
                        lonlat = (float(cache[ck_full][0]), float(cache[ck_full][1]))
                        cache_hits += 1
                    except Exception:
                        lonlat = None

                if lonlat is None:
                    gl, status, reason = geocode_address_mapbox(
                        token=token,
                        address=addr_raw,
                        city=city_raw,
                        state=state,
                        zipc=zipc,
                        sleep_s=max(0.0, args.geocode_sleep),
                    )
                    if gl:
                        lonlat = gl
                        n_geocoded += 1
                        cache[ck_full] = [float(gl[0]), float(gl[1])]
                        cache_writes += 1
                    else:
                        n_geocode_failed += 1
                        k = str(status)
                        fail_status_counts[k] = fail_status_counts.get(k, 0) + 1
                        if printed_failures < args.debug_geocode_failures:
                            printed_failures += 1
                            print(
                                f"⚠️  Geocode failed [{status}] {reason} :: "
                                f"{addr_raw}, {city_raw}, {state} {zipc} "
                                f"(Retailer: {retailer_raw})"
                            )

        if lonlat is not None:
            enriched.append({**props, "Longitude": str(lonlat[0]), "Latitude": str(lonlat[1])})
        else:
            unmatched.append(props)

    ensure_out_dir(args.out_dir)

    enriched_fields = [
        "Retailer","Suppliers","ContactName","ContactTitle","Title","Email","OfficePhone","CellPhone",
        "Address","City","State","Zip","Category","FacilityName","LongName","MatchTier","GeoSource",
        "Longitude","Latitude","FullAddress"
    ]
    write_csv(os.path.join(args.out_dir, "kingpin_enriched.csv"), enriched, enriched_fields)

    unmatched_fields = [
        "Retailer","Suppliers","ContactName","ContactTitle","Title","Email","OfficePhone","CellPhone",
        "Address","City","State","Zip","Category","FacilityName","LongName","MatchTier","GeoSource","FullAddress"
    ]
    write_csv(os.path.join(args.out_dir, "kingpin_unmatched.csv"), unmatched, unmatched_fields)

    features: List[Dict[str, Any]] = []
    for r in enriched:
        try:
            lon = float(r.get("Longitude", ""))
            lat = float(r.get("Latitude", ""))
        except Exception:
            continue
        props_out = {k: v for k, v in r.items() if k not in ("Longitude", "Latitude")}
        features.append(build_geojson_feature(lon, lat, props_out))

    out_doc = {"type": "FeatureCollection", "features": features}
    os.makedirs(os.path.dirname(args.out_geojson), exist_ok=True)
    with open(args.out_geojson, "w", encoding="utf-8") as f:
        json.dump(out_doc, f, ensure_ascii=False, indent=2)

    if args.geocode_no_match and cache_writes:
        save_geocode_cache(args.cache_file, cache)

    stats = {
        "input_rows_total_including_blanks": len(raw_rows),
        "rows_total_processed": n_total,
        "rows_dropped_blank": n_dropped_blank,
        "rows_dropped_missing_min": n_dropped_missing_min,
        "matched_t1": n_t1,
        "matched_t2": n_t2,
        "matched_t3": n_t3,
        "tbd_unmatched": n_tbd,
        "tbd_geocoded": n_geocoded,
        "tbd_geocode_failed": n_geocode_failed,
        "geocode_fail_status_counts": fail_status_counts,
        "cache_file": args.cache_file,
        "cache_hits": cache_hits,
        "cache_writes": cache_writes,
        "output_features": len(features),
        "out_geojson": args.out_geojson,
        "out_enriched_csv": os.path.join(args.out_dir, "kingpin_enriched.csv"),
        "out_unmatched_csv": os.path.join(args.out_dir, "kingpin_unmatched.csv"),
        "token_source": token_source,
    }

    with open(os.path.join(args.out_dir, "kingpin_build_stats.json"), "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print("✅ kingpin.geojson build complete")
    print(json.dumps(stats, indent=2))
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
